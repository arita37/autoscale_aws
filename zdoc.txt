
################################################################################
########## Main_server  ########################################################
batch_daemon_autoscale_cli.py :
   Copy tasks on S3 Drive
   Pull from github/tasks to S3 Drive

   Launch SPot Instance to run the tasks
   Send back task results from S3 to Github/task_out
   Close Spot Instnance


batch_daemon_launch_cli.py
   Launch the daemon to run the tasks.
     Scan each folder in zs3drive/tasks/
       Look for main.sh, task_config.py
       Execute main.sh
       
       Copy file back to zs3drive/tasks_out/
       Send results to github/tasks_out
   
   
   

batch_daemon_monitor_cli.py
   Monitor the tasks running from /tasks/ /main.sh
   
   











###############################################################################
###### Architecture
batch_daemon_launch_cli : Launch main.py from /tasks/xxx/ folder.
batch_daemon_monitor_cli : monitor the process launched from /tasks/

1 task is ONE folder in /tasks/ containing main.py or main.sh


Distributed version :
   Instances are spot launched from one standard AMI, share a common volume W,
   based on instance template.
   Each instance launch automatically batch_dameon_launch_cli, batch_dameon_monitor_cli
   

   Common drive W contains 3 folders  :
         /tasks/      :  folder
         /tasks_out/  :  output of task
         /zlog/


Auto-Scale :  
   batch_daemon_autoscale_cli.py (ONLY on master instance)
   Some basic rules like :
       nb_task_remaining > 10    AND   nb_CPU_available < 10 
          start new spot Instance  by AWS CLI.

       nb_task_remaining = 0 for last 5mins : 
          stop instance  by AWS CLI.
          

Instance :
  Instance are launched by AMI or instance template.
  Instance have 2 volumes :   
       root volume where conda, python, .... are installed.
       share volume W where tasks are stored.


Launch daemon :  cd /home/ubuntu/ && nohup  /home/ubuntu/zbatch.sh  >/dev/null 2>&1 &
Stop   daemon :  cd /home/ubuntu/ && /home/ubuntu/zbatch_kill.sh 


         
        
##############################################################################
Improvements toDO list
1) 
util_cpu.ps_find_procs_by_name(

Use RegEx to find this pattern 
"*tasks/*main.py" 
or "*tasks/*main.sh"

batch_daemon_launch_cli.py can DIE before batch_daemon_monitor_cli.py,
so we cannot use reference. Better to have loose couplings 
Ajey: Better to use system services like systemctl on ubuntu like systems.
yaki : Lets go for regex fitlering (logic is in the code, easier) 
       
       
2) Create
    batch_daemon_autoscale_cli.py
    Launch Spot Template / kill instance based on simple rules.
    Ajey: Rules are coded as part of the code, not read from configuration.
          Some basic rules like :
       nb_task_remaining > 10    AND   nb_CPU_available < 10 
          start new spot Instance  by AWS CLI.

       nb_task_remaining = 0 for last 5mins : 
          stop instance  by AWS CLI from master instance
          
    yaki :  yes, rules are Hard coded in the code.
            You can create simple Spot template from AMI.
            
            S3 was mounted as drive on /home/ubuntu\zs3drive/

    AMI to use for launching Spot
    Image: ami-005c8c01a65e8ada0


     Need to test concurrency on S3....


4) Clean the code of /batch/
    Ajey: let's target one file at a time, becoz the scope is very broad.
    yaki : /zarchive  /ztest can be removed.




####
https://www.ec2instances.info/?min_vcpus=8&region=us-west-2
a1.2xlarge  8cpu  0.128500 h
m5a.2xlarge  8cpu   0.13 USD
c5.4xlarge    16cpu   0.25 usd 
c5.9xlarge    36cpu   0.57 usd
m5a.12xlarge  40cpu   0.80usd














###############################################################################
###############################################################################
#######  Cloud9 usage #########################################################
### DO NOT restart the instance 
source activate py36
which python
which conda


zbatch.sh   : Launch the daemon processes
zbatch_kill :  Kill the 2 processes


#Root of github repo
cd  /home/ubuntu/aagit/aapackage/batch    
aapackage was installed in py36 conda env by :
pip install -e  .   (editable + packaging)



/home/ubuntu/tasks      :   Folder of the tasks
/home/ubuntu/zlog       :   Folder of the logs
/home/ubuntu/tasks_out  :   Folder of the tasks out


git add --all 
git commit -m "your msg"


topc    : Show all python processes running

gitpush :   send to github
gitpull :   get from github







"""
def batch_execute_parallel(HyperParametersFile,
                           subprocess_script, batch_log_file ="batch_logfile.txt",
                           waitseconds = 2):

    python_path = sys.executable

    HyperParameters  = pd.read_csv( HyperParametersFile)
    OptimizerName    = os.path.basename(subprocess_script)
    WorkingFolder = subprocess_script

    # logging.basicConfig(level=logging.INFO)
    #batch_log_file = os.path.join(WorkingFolder, "batch_logs/batch_%s.txt" % batch_label)
    ChildProcesses = []

    for ii in range(HyperParameters.shape[0]):
       # Extract parameters for single run from batch_parameters data.
       # params_dict = HyperParameters.iloc[ii].to_dict()
       log("Executing index", ii, WorkingFolder, "\n\n")

       proc = subprocess.Popen([ python_path, subprocess_script, str(ii) ],
                                 stdout=subprocess.PIPE)
       # ChildProcesses.append(proc)
"""






###############################################################################
###############################################################################
source activate py36

cd /home/ubuntu/

batch_daemon_launch_cli.py  --task_folder  tasks  --log_file   zlog/batchdaemong.log  --mode daemon  --waitsec 60  &

sleep 3

batch_daemon_monitor_cli.py --monitor_log_file zlog/process_log_file.log   --log_file zlog/batchdaemon_monitor.log    --mode daemon   --waitsec 20






###############################################################################
batch_daemon_launch_cli.py
   scans sub-folder in /tasks/
      and execute  /tasks/taskXXXX/main.py in that folder


batch_daemon_monitor_cli.py
   Monitor CPU / RAM usage on server


batch_local_aws_cli.py
  Launch AWS Batch
  and send task to aws folders
  and run on aws batch_monitor
  retrieve from aws to local




################################################################################
################################################################################
ENV install
    conda create -n testenv --yes $TO_INSTALL --file zbuild/py36.txt
    source activate testenv
    pip install arrow==0.10.0 attrdict==2.0.0 backports.shutil-get-terminal-size==1.0.0 configmy==0.14.87 github3.py==1.2.0 jwcrypto==0.6.0 kmodes==0.9 rope-py3k==0.9.4.post1 tables==3.3.0 tabulate==0.8.2 uritemplate==3.0.0
    pip install dcgpy

    conda install -c conda-forge  pygmo  pagmo




main.py
   1) Execute python script



main.py
   2) Mode parallel process :
       For each set of hyperparams,
          launch subprocess_optim

          from batch.util_batch import *
          batch_process(   )










### No need
3) Dynamically mount common volume w  to current instance :
   Using latest snapshot of the volume.
   
     ---> S3 was mounted as drive on
        /home/ubuntu\zs3drive/
        You can use it.
        
       We cannot drive too much intensive write/log.
       but, for batch write, read ,this is ok.
   

    




    One issue is the volume is saved by snapshot....
    When launching new instance how to pick the latest volume snapshot ???
    Ajey: Get the latest list of snapshots and use the latest one
    Yaki : There is issue of on-going writing.... 
           Look like very very difficult topic....
           
    
    Or maybe use S3 as disk storage ????
    https://cloud.netapp.com/blog/amazon-s3-as-a-file-system
    https://github.com/kahing/goofys
    https://objectivefs.com/why
    and create AMI snapshot
    Ajey: Havent tried this.
    









- Need to review functions :
    - util_batch.py :
        - batch_run_infolder :

        - batch_generate_hyperparameters :
            - df dataframe
            - need to check if hyper_dict has "key" key, and "min"/"max" keys for "key" key

        - batch_parallel_subprocess :
            - is csv really needed ? because, for each line in the csv file,
                subprocess_script is called with --hyperparam_ii={line_number}, why ?
                A for loop can do the job
                you seems not undertstanding the algorithm....
                Think of Nneural network hyperparametrization


    - util_log.py :
        - printlog :
            - if writelog:  alternative of logging for quick debugging, logging may have issues sometimes.





#############################################################################
ENV install
    conda create -n testenv --yes $TO_INSTALL --file zbuild/py36.txt
    source activate testenv
    pip install arrow==0.10.0 attrdict==2.0.0 backports.shutil-get-terminal-size==1.0.0 configmy==0.14.87 github3.py==1.2.0 jwcrypto==0.6.0 kmodes==0.9 rope-py3k==0.9.4.post1 tables==3.3.0 tabulate==0.8.2 uritemplate==3.0.0


    conda install -c conda-forge  pygmo  pagmo
    pip install dcgpy







###############################################################################
batch_daemon_launch_cli.py
   scans sub-folder in /tasks/
      and execute  /tasks/taskXXXX/main.py in that folder


main.py
   1) Execute python script



main.py
   2) Mode parallel process :
       For each set of hyperparams,
          launch subprocess_optim

          from batch.util_batch import *
          batch_process(   )


batch_daemon_monitor_cli.py
   Monitor CPU / RAM usage on server



batch_launch_aws_cli.py
  Launch AWS Batch
  and send task to aws folders


































































meta_batch_task.py :
   ...task_folder/task1/mybatch_optim.py  + hyperparamoptim.csv +  myscript.py ...
   ...task_folder/task2/mybatch_optim.py  + hyperparamoptim.csv + .myscript.py ...


meta_batch_task.py :
   for all "task*" folders in task_working_dir  :
       run subprocess taskXXX/mybatch_optim.py



mybatch_optim.py :
  for all rows ii of hyperparams.csv
     run subprocess  myscript.py  ii
     check CPU suage with psutil.   CPU usage < 90 %   mem usage < 90%


### not needed from you
aws_batch_script.py :
   for all tasks folder in LOCAL PC :
       transfer by SFTP to REMOTE Task folder (by zip)

   subprocess  meta=batch_task.py on REMOTE PC.


##### Warning :
  Please check my initial code to make sure most of functionnalities are here in your code


####################################################################
batch_sequencer.py :
  Launch 1 subprcoess per hyperparam row.










##################################################################################
Task 1 :
  Create/Update large scale Genetic OPtimizer batch


### Existing code is here:
  https://github.com/arita37/a_aapackage/tree/master/batch/batch_aws/task/elvis_prod_20161231/



Launch the batcher for each set of hyper-parameters.
  pygmo_batch_generic.py

Launch 1 single Optimization for 1 hyper parameters
  elvis_pygmo_optim.py


A)
  this fileis using old pygmo
  code should be updated to use new pygmo 2.0


B)
  Current batch code is quite ad hoc...
  would like to make it generic as follow :
     function to be optimized in one file.
     list of hyper parameters into a csv file

     for EACH set of hyper-parameters (1 line of pandas csv file) :
        1 subprocess is launched and optimization done by  pygmo
        result are saved somewhere.




###################################################################################################
######### Architecture :  #########################################################################
1)  mybatch.py
  from aapackage.batch import batch

  batch.batch_run(
      file_script      = "folder/myscript_optim.py",
      hyperparam_file  = "hyperparam_list.csv",
      out_folder       = "/myresult/batch1/",
      schedule         = "11/02/2019 12:30"
  )  :

      for ii, row of df_hyperparam_list.iterrows() ,
          pid = subprocess  row["file_script"]  ii  "hyperparam_list.csv"  out_folder   ###All subprocess are independdant !!!!
          server_info = "_".join( util.os_server_info())
          logs(arrow.timestamp, pid, server_info,  row["file_script"]  ii  hyperparam_file )
          wait(waitsecs)

  batch.monitor_pid(auto_close=True)  ### Auto close the EC2 instance when no PID is running.




3) myscript_optim.py  ii  hyperparam_file
   from aapackage import optim  ### Layer above pygmo to abstract under functional form.

   df = pd.csv_read( args.hyperparam_file )
   df = df.iloc[ii, :]  # only 1 line

   import_load(  df["myfun_file"].values )  as myfun_file # "folder/myfun.py"   string load of module.

   optim1 = optim.optim(
              fun_eval=     myfun_file.mypersoname_eval,
              fun_bound=    myfun_file.mypersoname_bound,
              fun_gradient= None,
              algo= "de_1227",
              algo_origin="scipy/pygmo/...",
              dict_params = df.to_dict()
            )

   res = optim1.optimize()
   vv  = res.dump()
   util.logs( vv , type="csv")



3) User define function is defined into a python file : myfun.py
  myfun_file.py

     def mypersoname_eval_()
        return np.float    or vector of float

     def mypersoname_bound_()
       return [(-0.1 , 0.4)   ,  (0.1 , 0.4)   ]

     def fun_gradient_()



####################################################################################
#### Version with auto-launch of AWS  ##############################################
0) mybatch_ssh.py
  import util_aws

  ip0 = util_aws.aws_launch()  # start ec2

  ssh0 = ssh(ip0, ...)
  ssh.put( from="local/task/", to="ec2/task/"  )

  ssh.exec( "python mybatch.py"  )   ####


















  from myscript import fun_eval, boun


















